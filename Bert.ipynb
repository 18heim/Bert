{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import tensorflow.keras.backend as K\n",
    "import transformers\n",
    "from transformers import TFCamembertModel\n",
    "from transformers import CamembertTokenizer\n",
    "from ipywidgets import IntProgress\n",
    "\"\"\"Probably should try with TFCamemForSequenceClassification\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "\n",
    "MAX_LEN = 2000\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('../CNN-kim/Data_2labels_CIB4/df_test_clean.pickle','rb') as fichier:\n",
    "    df_test = pickle.load(fichier)\n",
    "\n",
    "with open ('../CNN-kim/Data_2labels_CIB4/df_train_clean.pickle','rb') as fichier:\n",
    "    df = pickle.load(fichier)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (3751492, 5)\n",
      "TRAIN Dataset: (3001194, 5)\n",
      "TEST Dataset: (750298, 5)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "df_train = df.sample(frac=train_size, random_state=200)\n",
    "df_dev = df.drop(df_train.index).reset_index(drop=True)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(df_train.shape))\n",
    "print(\"TEST Dataset: {}\".format(df_dev.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_camembert_tfdataset(df):\n",
    "    labels = list(df['cat'])\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for text in df['text']:\n",
    "        inputs = tokenizer.encode_plus(text,\n",
    "                                     add_special_tokens=True,\n",
    "                                     max_length=MAX_LEN,\n",
    "                                     pad_to_max_length=True,\n",
    "                                     return_token_type_ids=True,\n",
    "                                     return_tensors=\"np\",\n",
    "                                     truncation=True,\n",
    "                                     )\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        attention_mask.append(inputs['attention_mask'])\n",
    "        \n",
    "    return (input_ids,attention_mask,labels)\n",
    "                                             \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids,attention_mask,labels = convert_to_camembert_tfdataset(df_train[:200000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2000)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(input_ids[0].shape)\n",
    "print(labels[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _input_fn():\n",
    "    def generator():\n",
    "        for ids, mask, l in zip(input_ids, attention_mask, labels):\n",
    "            yield {\"input_id\" : ids, \"input_mask\" : mask }, l\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(generator,\n",
    "                                    output_types= ({\"input_id\" :tf.int32, \"input_mask\" : tf.int32},tf.int32),                            \n",
    "                                    output_shapes= ({\"input_id\" : (2000), \"input_mask\" : (2000)}, (1))\n",
    "                                            )\n",
    "                            \n",
    "                                                        \n",
    "                                            \n",
    "    dataset = dataset.batch(8)\n",
    "    return(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({\"input_id\" : }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = convert_to_camembert_tfdataset(df_train[:200000])\n",
    "dev_dataset = convert_to_camembert_tfdataset(df_dev[:100000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32, name='input_id')\n",
    "    mask = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32, name='input_mask')\n",
    "    camembert_model = TFCamembertModel.from_pretrained('camembert-base', from_pt=True)\n",
    "    \n",
    "    camembert_layer = camembert_model([ids,mask])[0]\n",
    "    flattened_layer = tf.keras.layers.Flatten()(camembert_layer)\n",
    "    dropout_layer = tf.keras.layers.Dropout(0.2)(flattened_layer)\n",
    "    \n",
    "    dense_output_layer = tf.keras.layers.Dense(units=435,\n",
    "                                              activation= 'softmax')(dropout_layer)\n",
    "    model = tf.keras.models.Model(inputs=[ids,mask], outputs=[dense_output_layer])\n",
    "    \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss = loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return(model)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFCamembertModel.\n",
      "\n",
      "Some weights or buffers of the PyTorch model TFCamembertModel were not initialized from the TF 2.0 model and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\transformers\\modeling_tf_roberta.py:231 call  *\n        outputs = self.roberta(inputs, **kwargs)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:773 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\transformers\\modeling_tf_bert.py:606 call  *\n        embedding_output = self.embeddings([input_ids, position_ids, token_type_ids, inputs_embeds], training=training)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:822 __call__\n        outputs = self.call(cast_inputs, *args, **kwargs)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\transformers\\modeling_tf_bert.py:166 call\n        return self._embedding(inputs, training=training)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\transformers\\modeling_tf_roberta.py:99 _embedding\n        return super()._embedding([input_ids, position_ids, token_type_ids, inputs_embeds], training=training)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\transformers\\modeling_tf_bert.py:193 _embedding\n        embeddings = self.LayerNorm(embeddings)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:773 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\normalization.py:1080 call\n        scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\normalization.py:1073 _broadcast\n        return array_ops.reshape(v, broadcast_shape)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py:193 reshape\n        result = gen_array_ops.reshape(tensor, shape, name)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py:7442 reshape\n        \"Reshape\", tensor=tensor, shape=shape, name=name)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py:742 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py:595 _create_op_internal\n        compute_device)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:3322 _create_op_internal\n        op_def=op_def)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1786 __init__\n        control_input_ops)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1622 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Cannot reshape a tensor with 768 elements to shape [1,1,2000,1] (2000 elements) for 'tf_camembert_model_5/roberta/embeddings/LayerNorm/Reshape' (op: 'Reshape') with input shapes: [768], [4] and with input tensors computed as partial shapes: input[1] = [1,1,2000,1].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-a3add9c3d06c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m ''' - Bon en gros les données va falloir que je les tokenize ligne par ligne, c'est chiant, et ensuite je pourrais voir mon\n\u001b[0;32m      4\u001b[0m \u001b[0mentrée\u001b[0m \u001b[0mà\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m regarder si le Sequenceclassification model est bien aussi ou pas. '''\n",
      "\u001b[1;32m<ipython-input-21-b7540592a834>\u001b[0m in \u001b[0;36mbuild_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcamembert_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTFCamembertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'camembert-base'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_pt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mcamembert_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcamembert_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mflattened_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcamembert_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdropout_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflattened_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    771\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[0;32m    772\u001b[0m                   \u001b[1;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m                     \u001b[1;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m                     \u001b[1;31m# circular dependencies.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in converted code:\n\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\transformers\\modeling_tf_roberta.py:231 call  *\n        outputs = self.roberta(inputs, **kwargs)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:773 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\transformers\\modeling_tf_bert.py:606 call  *\n        embedding_output = self.embeddings([input_ids, position_ids, token_type_ids, inputs_embeds], training=training)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:822 __call__\n        outputs = self.call(cast_inputs, *args, **kwargs)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\transformers\\modeling_tf_bert.py:166 call\n        return self._embedding(inputs, training=training)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\transformers\\modeling_tf_roberta.py:99 _embedding\n        return super()._embedding([input_ids, position_ids, token_type_ids, inputs_embeds], training=training)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\transformers\\modeling_tf_bert.py:193 _embedding\n        embeddings = self.LayerNorm(embeddings)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:773 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\normalization.py:1080 call\n        scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\normalization.py:1073 _broadcast\n        return array_ops.reshape(v, broadcast_shape)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py:193 reshape\n        result = gen_array_ops.reshape(tensor, shape, name)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py:7442 reshape\n        \"Reshape\", tensor=tensor, shape=shape, name=name)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py:742 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py:595 _create_op_internal\n        compute_device)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:3322 _create_op_internal\n        op_def=op_def)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1786 __init__\n        control_input_ops)\n    C:\\Users\\cheim\\Anaconda3\\envs\\camemBert\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1622 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Cannot reshape a tensor with 768 elements to shape [1,1,2000,1] (2000 elements) for 'tf_camembert_model_5/roberta/embeddings/LayerNorm/Reshape' (op: 'Reshape') with input shapes: [768], [4] and with input tensors computed as partial shapes: input[1] = [1,1,2000,1].\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit([input_ids,attention_mask], labels, epochs=2, batch_size=10)\n",
    "''' - Bon en gros les données va falloir que je les tokenize ligne par ligne, c'est chiant, et ensuite je pourrais voir mon\n",
    "entrée à model.fit\n",
    "regarder si le Sequenceclassification model est bien aussi ou pas. '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
